% \VignetteIndexEntry{Introduction to doMPI}
% \VignetteDepends{doMPI}
% \VignettePackage{doMPI}
\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage[pdftex]{graphicx}
\usepackage{color}
\usepackage{xspace}
\usepackage{fancyvrb}
\usepackage{fancyhdr}
\usepackage[
         colorlinks=true,
         linkcolor=blue,
         citecolor=blue,
         urlcolor=blue]
         {hyperref}
         \usepackage{lscape}
\usepackage{Sweave}           

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% define new colors for use
\definecolor{darkgreen}{rgb}{0,0.6,0}
\definecolor{darkred}{rgb}{0.6,0.0,0}
\definecolor{lightbrown}{rgb}{1,0.9,0.8}
\definecolor{brown}{rgb}{0.6,0.3,0.3}
\definecolor{darkblue}{rgb}{0,0,0.8}
\definecolor{darkmagenta}{rgb}{0.5,0,0.5}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\newcommand{\bld}[1]{\mbox{\boldmath $#1$}}
\newcommand{\shell}[1]{\mbox{$#1$}}
\renewcommand{\vec}[1]{\mbox{\bf {#1}}}

\newcommand{\ReallySmallSpacing}{\renewcommand{\baselinestretch}{.6}\Large\normalsize}
\newcommand{\SmallSpacing}{\renewcommand{\baselinestretch}{1.1}\Large\normalsize}

\newcommand{\halfs}{\frac{1}{2}}

\setlength{\oddsidemargin}{-.25 truein}
\setlength{\evensidemargin}{0truein}
\setlength{\topmargin}{-0.2truein}
\setlength{\textwidth}{7 truein}
\setlength{\textheight}{8.5 truein}
\setlength{\parindent}{0.20truein}
\setlength{\parskip}{0.10truein}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\pagestyle{fancy}
\lhead{}
\chead{Introduction to {\em doMPI}}
\rhead{}	
\lfoot{}
\cfoot{}
\rfoot{\thepage}
\renewcommand{\headrulewidth}{1pt}
\renewcommand{\footrulewidth}{1pt}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Introduction to {\tt doMPI}}
\author{Steve Weston \\ stephen.b.weston@gmail.com}


\begin{document}

\maketitle

\thispagestyle{empty}
	
\section{Introduction}

<<loadLibs,results=hide,echo=FALSE>>=
library(doMPI)
@

The \texttt{doMPI} package is what I call a ``parallel backend'' for the
\texttt{foreach} package.  Since the \texttt{foreach} package is not a
parallel programming system, but a parallel programming framework, it
needs a parallel programming system to do the actual work in parallel.
The \texttt{doMPI} package acts as an adaptor to the \texttt{Rmpi}
package, which in turn is an \texttt{R} interface to an implementation
of \texttt{MPI}.  \texttt{MPI}, or \emph{Message Passing Interface}, is
a specification for an \texttt{API} for passing messages between
different computers.  There are a number of \texttt{MPI} implementations
available that allow data to be moved between computers quite
efficiently.

Programming with \texttt{MPI} is rather difficult, however, since it is
a rather large and complex \texttt{API}.  For example, the \texttt{Rmpi}
package defines about 110 \texttt{R} functions, only a few of which are
only for internal use.  And the \texttt{MPI} standard includes many more
functions that aren't supported by \texttt{Rmpi}, such as the file
functions.  Of course, you only need to learn about a small percentage
of those functions in order to start using \texttt{MPI} effectively, but
it can take awhile just to figure out which functions are really
important, and which ones you can safely ignore.

The \texttt{foreach} package is an attempt to make parallel computing
much simpler by providing a parallel for-loop construct for \texttt{R}.
As an adaptor to \texttt{MPI}, the \texttt{doMPI} package is an attempt
to give you the best of both worlds: the ease of use of parallel
for-loops, with the efficiency of \texttt{MPI}.

Unfortunately, there are still a wide variety of problems that you may
run into.  First, you need to have \texttt{MPI} installed on your
computers, and then you have to build and install \texttt{Rmpi} to use
that \texttt{MPI} installation.  That is where many people run into
problems, particularly on Windows.  On Ubuntu, it is very easy to
install \texttt{OpenMPI}, and \texttt{Rmpi} works quite well with it.
Mac OS X 10.5 and later includes \texttt{OpenMPI} with the developer
tools, and \texttt{Rmpi} works well with that, also.

Another think to be aware of is that you have to run your \texttt{R}
programs differently in order to execute on a computer cluster, and the
exact method varies depending on what \texttt{MPI} implementation was
used to build your installation of \texttt{Rmpi}.  If you just start a
normal \texttt{R} session, your workers will probably only run on your
local machine.  That's great for testing, but the point of using
\texttt{Rmpi} is to use multiple machines, and perhaps a cluster.  To do
that, you'll need to start your \texttt{R} session using a comman such
as orterun, mpirun, or mpiexec, depending on your \texttt{MPI}
installation.

Hopefully, this document will help to get you started, but it primarily
deals with \texttt{foreach} and \texttt{doMPI}, and for running
\texttt{doMPI} scripts, I'll only discuss the use of \texttt{OpenMPI}.
Very likely you will have some trouble that I can't help you with, and
so I highly recommend that you join the R-sig-hpc mailing list.  It is a
very friendly and helpful community, and there isn't even a lot of
traffic on it.

\section{Installing the software}

Assuming that you already have \texttt{R} and \texttt{MPI} installed on
your computers, you will need to install \texttt{doMPI} and all of it's
dependencies.  That could be done with a single ``install.packages''
command, but you might want to explicityly install \texttt{Rmpi} first,
just so you can clearly see whether it gets installed correctly or not.
On Ubuntu, you can actually install \texttt{Rmpi} by using
\texttt{apt-get} to install the Debian \texttt{r-cran-rmpi} package.
That's the most fool-proof way to install it on Ubuntu.  But on other
systems, including Mac OS X, you can install it with:

<<install, eval=FALSE>>=
install.packages("Rmpi")
@

As I mentioned, if you have problems, I strongly recommend the R-sig-hpc
mailing list if you don't have a local expert.  I'm not an expert on
installing \texttt{Rmpi}, since I've only ever installed it on Ubuntu
and Mac OS X.

Once \texttt{Rmpi} is installed, the rest should be easy:

<<install, eval=FALSE>>=
install.packages("doMPI", dependencies=TRUE)
@

That will also install packages such as \texttt{foreach} and
\texttt{iterators}.  You may also want to install the multicore package,
which can be used by \texttt{doMPI}, but it is not required.  Oddly
enough, it is also possible to use \texttt{doMPI} with the nws package,
but again, it's not required.

\section{Getting Started}

So if I haven't scared you off with my introduction, and you got all of
the software installed, then it's time to see if it actually works!  For
testing purposes, we'll just run on a single computer.  In that case,
you just start a normal \texttt{R} session.  I'll talk about running on
multiple computers later.

Once you've started an \texttt{R} session, load \texttt{doMPI} as normal:

<<>>=
library(doMPI)
@

You'll get an error if \texttt{Rmpi} isn't properly installed.  If that
happens, you should restart the \texttt{R} session, load \texttt{Rmpi}
directly, write down the exact error message, google around for the
error, scratch your head for awhile, and then post a polite,
well-thought-out request for help to R-sig-hpc.  But don't be too
surprised if someone\footnote{Like Dirk, for example.} suggests that you
switch to Ubuntu.

Once you can successfully load \texttt{doMPI}, the next step is to
create an \texttt{MPI} cluster object.  There are lots of options, but
here's one way to do that:

<<>>=
cl <- startMPIcluster(count=2)
@

As you can see from the friendly message, this starts two cluster
workers, or slaves, as \texttt{MPI} calls them.  Note that if you're
using \texttt{OpenMPI}, those two processes will immediately start using
as much of your CPU as they can.  This is a known issue with
\texttt{OpenMPI}.  They are waiting for the master process to send them
a message, but they are ``busy waiting'' for that message.  That doesn't
make much sense in this context, but it can give better performance in
some contexts.  At any rate, it is an issue that the \texttt{OpenMPI}
group is planning to address in a future release.

The next step is to register the \texttt{MPI} cluster object with the
\texttt{foreach} package.  This is done with the ``registerDoMPI'':

<<>>=
registerDoMPI(cl)
@

This tells \texttt{foreach} that you want to use \texttt{doMPI} as the
parallel backend, and that you want to use the specified \texttt{MPI}
cluster object to execute the tasks.  If you don't register a cluster
object, your tasks won't be executed in parallel, even though you use
\texttt{foreach} with the \%dopar\% operator.

\section{A simple example}

Now we're ready to run a little test that actually runs in parallel.  I
always do that with a very simple program that executes the
\texttt{sqrt} function in parallel.  It's not a practical use of
\texttt{foreach} because the \texttt{sqrt} function runs so fast that it
makes no sense to execute it in parallel.  But it makes a great test.

<<>>=
foreach(i=1:3) %dopar% sqrt(i)
@

There are a lot of things to comment on in this little example.  First
of all, notice that a list containing three numbers is returned by this
command.  That is quite different from a for-loop.  If we did this in a
for-loop, we would execute \texttt{sqrt} three times, but the result
would be thrown away.  A for-loop normally uses assignments to save its
results, but \texttt{foreach} works more like \texttt{lapply} in this
respect, and that is what makes it work well for parallel execution.

Secondly, notice that we're using a strange binary operator, named
\texttt{\%dopar\%}.  \texttt{R} doesn't really make it easy to define
new control structures, but since arguments to functions are lazily
evaluated, it is possible.  You don't have to worry about that, however.
Just keep in mind that it needs to be there.

In a real program, you'll obviously want to save the results of the
computations.  You may also have several operations to perform, so the
way you'll usually write the previous example is as:

<<>>=
x <- foreach(i=1:3) %dopar% {
    sqrt(i)
}
x
@

Now if you squint a bit it will look just like a for-loop.

\section{The .combine option}

But what if we want our results to be returned in a vector, rather than
a list?  We could convert it to a list afterwards, but there's another
way.  The \texttt{foreach} function takes a number of addition
arguments, all of which start with a ``.'' to distinguish them from the
``variable'' arguments.  The one we need in this case is named
\texttt{.combine}.  You can use it to specify a function that will
combine the results.  This function must take at least two input
arguments, and return a value which is a ``combined'' or possibly
``reduced'' version of its input arguments.  To combine many numeric
values into a single vector of numeric values, we can use the standard
\texttt{c} function, which concatenates its arguments:

<<>>=
x <- foreach(i=1:3, .combine="c") %dopar% {
    sqrt(i)
}
x
@

This works well for numeric, character, and logical values.

I also use the \texttt{cbind} function as a \texttt{.combine} function
quite often.  I use it to turn vectors into matrices:

<<>>=
x <- foreach(seed=c(7, 11, 13), .combine="cbind") %dopar% {
    set.seed(seed)
    rnorm(3)
}
x
@

\section{Computing the sinc function}

Now let's try a slightly more realistic example.  Let's evaluate the
\texttt{sinc} function over a grid of values.  We'd like to return the
results in a matrix, as in the previous example, so we'll use
\texttt{cbind} to combine the results:

<<fig=TRUE>>=
x <- seq(-8, 8, by=0.5)
v <- foreach(y=x, .combine="cbind") %dopar% {
    r <- sqrt(x^2 + y^2 + .Machine$double.eps)
    sin(r) / r
}
persp(x, x, v)
@

One thing to note in this example is that I'm doing an assignment to the
variable \texttt{r} in the loop.  You can do that with \texttt{foreach},
but if you want your code to work correctly in parallel, you need to be
careful not to use variable assignments as a way of communicating
between different iterations of the loop.  That's what parallel
programmers call a ``loop dependence''.  In this case I'm only using it
to communicate within a single iteration, so it's safe.

Also note that I'm using the variable \texttt{x} inside the loop, which
was defined before the loop.  In many other parallel computing systems,
you'd have to explicitly export that variable somehow or other.  With
\texttt{foreach}, it's done automatically.

As a comparison, let's write this program using the standard
\texttt{lapply} function:

<<results=hide>>=
x <- seq(-8, 8, by=0.5)
sinc <- function(y) {
    r <- sqrt(x^2 + y^2 + .Machine$double.eps)
    sin(r) / r
}
r <- lapply(x, sinc)
v <- do.call("cbind", r)
persp(x, x, v)
@

As you can see, there is a similarity between these two versions.  But
it makes me very happy that the parallel version is actually shorter and
easier to explain than the sequential version.

I should also point out that in this \texttt{sinc} example, I've been
very careful to make use of vector operations.  The primary rule for
getting good performance in \texttt{R} is to use vector operations
whenever possible.  I flagrantly broke that rule in the \texttt{sqrt}
example.  In the \texttt{sinc} example, I used vector operations to
compute the columns, and I used \texttt{foreach} to execute those vector
operations in parallel.  The \texttt{sinc} example is still not really
compute intensive enough to really merit executing it in parallel, but
it's a much better example use of \texttt{foreach} than the
\texttt{sqrt} example.

\section{Processing a directory of data files}

Now let's try a different kind of example that you probably haven't seen
in any parallel programming book, but simply reeks of practicality.
Let's say that you have a directory full of CSV data files, and you want
to read each of them, analyze the data in some way, and produce a plot
of the results that you write to output files.  Here I'll do that using
the \texttt{randomForest} package:

<<results=hide>>=
ifiles <- list.files(pattern="\\.csv$")
ofiles <- sub("\\.csv$", ".png", ifiles)
foreach(i=ifiles, o=ofiles, .packages="randomForest") %dopar% {
    d <- read.csv(i)
    rf <- randomForest(Species~., data=d, proximity=TRUE)
    png(filename=o)
    MDSplot(rf, d$Species)
    dev.off()
    NULL
}
@

Note that we're iterating over two different vectors in this example:
\texttt{ifiles} and \texttt{ofiles}, which are both the same length.
The \texttt{foreach} function let's you specify any number of variables
to iterate over, and it stops when one of them runs out of values.

Also note the \texttt{NULL} at the end of the loop body.  There is no
value to return in this case, so I used a \texttt{NULL} to make sure
that I wasn't needlessly sending anything large back to the master,
which would hurt the performance somewhat.  The real results of
executing the loop are written to disk by the workers directly.

\section{Running doMPI on a cluster}

Now that you've run some interesting examples on a single computer,
let's try running on multiple computers, which is really the purpose of
the \texttt{doMPI} package.

[Describe how to use the orterun command.]

\section{Conclusion}

There's a lot more that I could show you about \texttt{foreach}, but
this is a good place to stop, because at the moment I want to convince
you that it is easy to use \texttt{foreach} for basic parallel
programming tasks.  On another day I may try to convince you that you
can do rather sophisticated tasks, also.

So if you haven't done it yet, install \texttt{doMPI} and start playing
around with it.  If you are using Windows, you can just install the
\texttt{foreach} and \texttt{iterators} packages and still run all of
these examples.  You just won't be running in parallel.  You'll get a
warning that you're running sequentially, but you should get exactly the
same results.  But that will allow you to develop parallel applications
on your Windows machine, which you can later deploy on a Linux cluster.

Have fun!

\end{document}
